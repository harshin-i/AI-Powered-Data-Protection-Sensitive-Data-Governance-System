{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481ed217",
   "metadata": {},
   "source": [
    "\n",
    "# Complete Data Protection Project — Full BERT Training Version (Option B)\n",
    "\n",
    "This notebook contains the **full pipeline**:\n",
    "1. Full DistilBERT fine-tuning (production-style). Use `QUICK_RUN=False` for full training.\n",
    "2. Integration pipeline — hybrid regex + BERT inference, masking/encryption, policy validation.\n",
    "3. Anomaly detection, reports generation.\n",
    "4. Auto-creates Streamlit dashboard at `dashboards/streamlit_app.py`.\n",
    "\n",
    "**Important notes before running**:\n",
    "- This **full** notebook will download transformer weights and can be slow on CPU. Use a GPU runtime if possible.\n",
    "- Set `QUICK_RUN=True` to run a small quick demo (1 epoch, small subset). For full training, set `QUICK_RUN=False` and ensure you have sufficient RAM and GPU.\n",
    "- Ensure `data/data_protection_dataset.csv` exists in the project folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3ffead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed dependencies (or already present).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install dependencies (run once)\n",
    "!pip install -q transformers datasets torch scikit-learn pandas joblib cryptography pyyaml tqdm streamlit accelerate\n",
    "print('Installed dependencies (or already present).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f35c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch 2.9.1+cpu CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys, json, math, random, time\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# allow importing from src/\n",
    "sys.path.append(os.path.abspath('./src'))\n",
    "\n",
    "print('Torch', torch.__version__, 'CUDA available:', torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6c647a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: data/data_protection_dataset.csv\n",
      "MODEL_DIR: models/bert_sensitivity\n",
      "QUICK_RUN: False EPOCHS: 3 BATCH_SIZE: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Settings - change as needed\n",
    "DATA_PATH = 'data/data_protection_dataset.csv'\n",
    "MODEL_DIR = 'models/bert_sensitivity'\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('dashboards', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "QUICK_RUN = False   # False = full training (Option B). Set True for quick demo.\n",
    "NUM_EPOCHS = 3 if not QUICK_RUN else 1\n",
    "BATCH_SIZE = 16 if torch.cuda.is_available() else 8\n",
    "MAX_LEN = 128\n",
    "\n",
    "print('DATA_PATH:', DATA_PATH)\n",
    "print('MODEL_DIR:', MODEL_DIR)\n",
    "print('QUICK_RUN:', QUICK_RUN, 'EPOCHS:', NUM_EPOCHS, 'BATCH_SIZE:', BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c89b1cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>phone</th>\n",
       "      <th>aadhaar</th>\n",
       "      <th>pan</th>\n",
       "      <th>address</th>\n",
       "      <th>transaction_amount</th>\n",
       "      <th>access_time</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>event_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yuvaan Shetty</td>\n",
       "      <td>sidhudhruv@devi.com</td>\n",
       "      <td>195064218</td>\n",
       "      <td>5139 5713 7713</td>\n",
       "      <td>FFTIV8802J</td>\n",
       "      <td>H.No. 22, Saran Circle, Saharanpur-533322</td>\n",
       "      <td>38316.57</td>\n",
       "      <td>2024-01-03 22:53:00</td>\n",
       "      <td>63.223.125.68</td>\n",
       "      <td>LOGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yashvi Ahluwalia</td>\n",
       "      <td>dsom@yadav.com</td>\n",
       "      <td>914272822415</td>\n",
       "      <td>3666 6865 8017</td>\n",
       "      <td>KOPMD3006N</td>\n",
       "      <td>11/21, Tripathi Nagar, Jaipur 804158</td>\n",
       "      <td>33660.28</td>\n",
       "      <td>2024-01-07 07:30:00</td>\n",
       "      <td>200.74.114.217</td>\n",
       "      <td>LOGOUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jivika Sachar</td>\n",
       "      <td>qravi@bath.biz</td>\n",
       "      <td>49274419</td>\n",
       "      <td>7261 9935 3973</td>\n",
       "      <td>FBLLW7021X</td>\n",
       "      <td>608, Choudhary Ganj, Howrah 353910</td>\n",
       "      <td>21059.80</td>\n",
       "      <td>2024-01-01 08:04:00</td>\n",
       "      <td>181.5.110.179</td>\n",
       "      <td>FAILED_LOGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aradhya Magar</td>\n",
       "      <td>cmaharaj@din.com</td>\n",
       "      <td>918653184494</td>\n",
       "      <td>2112 2893 5177</td>\n",
       "      <td>YXPFE7972M</td>\n",
       "      <td>66/736, Bawa Nagar, Yamunanagar 938547</td>\n",
       "      <td>19773.25</td>\n",
       "      <td>2024-02-04 09:51:00</td>\n",
       "      <td>208.88.77.128</td>\n",
       "      <td>LOGIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tara Cherian</td>\n",
       "      <td>wagleritvik@raja.org</td>\n",
       "      <td>5066730025</td>\n",
       "      <td>9895 5214 3164</td>\n",
       "      <td>GPAMD5692N</td>\n",
       "      <td>H.No. 931, Chaudhari Path, Nizamabad 926737</td>\n",
       "      <td>36964.10</td>\n",
       "      <td>2024-02-04 08:11:00</td>\n",
       "      <td>77.137.248.169</td>\n",
       "      <td>DELETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                 email         phone         aadhaar  \\\n",
       "0     Yuvaan Shetty   sidhudhruv@devi.com     195064218  5139 5713 7713   \n",
       "1  Yashvi Ahluwalia        dsom@yadav.com  914272822415  3666 6865 8017   \n",
       "2     Jivika Sachar        qravi@bath.biz      49274419  7261 9935 3973   \n",
       "3     Aradhya Magar      cmaharaj@din.com  918653184494  2112 2893 5177   \n",
       "4      Tara Cherian  wagleritvik@raja.org    5066730025  9895 5214 3164   \n",
       "\n",
       "          pan                                      address  \\\n",
       "0  FFTIV8802J    H.No. 22, Saran Circle, Saharanpur-533322   \n",
       "1  KOPMD3006N         11/21, Tripathi Nagar, Jaipur 804158   \n",
       "2  FBLLW7021X           608, Choudhary Ganj, Howrah 353910   \n",
       "3  YXPFE7972M       66/736, Bawa Nagar, Yamunanagar 938547   \n",
       "4  GPAMD5692N  H.No. 931, Chaudhari Path, Nizamabad 926737   \n",
       "\n",
       "   transaction_amount          access_time      ip_address    event_type  \n",
       "0            38316.57  2024-01-03 22:53:00   63.223.125.68         LOGIN  \n",
       "1            33660.28  2024-01-07 07:30:00  200.74.114.217        LOGOUT  \n",
       "2            21059.80  2024-01-01 08:04:00   181.5.110.179  FAILED_LOGIN  \n",
       "3            19773.25  2024-02-04 09:51:00   208.88.77.128         LOGIN  \n",
       "4            36964.10  2024-02-04 08:11:00  77.137.248.169        DELETE  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      " label\n",
      "2    200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Place your CSV there.\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Rows:', len(df))\n",
    "display(df.head())\n",
    "\n",
    "# Build text column\n",
    "def make_text(row):\n",
    "    parts = []\n",
    "    for c in ['name','email','phone','address','event_type']:\n",
    "        v = row.get(c, '') if pd.notna(row.get(c,'')) else ''\n",
    "        if v and str(v).strip():\n",
    "            parts.append(str(v))\n",
    "    parts.append(f\"Transaction INR {row.get('transaction_amount','')}\")\n",
    "    return ' | '.join(parts)\n",
    "\n",
    "df['text'] = df.apply(make_text, axis=1)\n",
    "\n",
    "# Heuristic labels (for supervised fine-tuning)\n",
    "def label_row(row):\n",
    "    if pd.notna(row.get('aadhaar')) and str(row.get('aadhaar')).strip():\n",
    "        return 2\n",
    "    if pd.notna(row.get('pan')) and str(row.get('pan')).strip():\n",
    "        return 2\n",
    "    if pd.notna(row.get('email')) and str(row.get('email')).strip():\n",
    "        return 1\n",
    "    if pd.notna(row.get('phone')) and str(row.get('phone')).strip():\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df['label'] = df.apply(label_row, axis=1)\n",
    "print('Label distribution:\\n', df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b837ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer ready. Example tokenization:\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "print('Tokenizer ready. Example tokenization:')\n",
    "print(tokenizer('Hello world', truncation=True, return_tensors='pt')['input_ids'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d6ae520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer configured. EPOCHS: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train/validation split (stratified)\n",
    "train_df, val_df = train_test_split(df[['text','label']], test_size=0.15, random_state=42, stratify=df['label'])\n",
    "\n",
    "if QUICK_RUN:\n",
    "    train_df = train_df.sample(min(500, len(train_df)), random_state=42)\n",
    "    val_df = val_df.sample(min(200, len(val_df)), random_state=42)\n",
    "\n",
    "train_dataset = TextDataset(train_df['text'], train_df['label'], tokenizer, max_len=MAX_LEN)\n",
    "val_dataset = TextDataset(val_df['text'], val_df['label'], tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "num_labels = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro' if not QUICK_RUN else 'accuracy',\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, preds), 'f1_macro': f1_score(labels, preds, average='macro')}\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metrics)\n",
    "print('Trainer configured. EPOCHS:', NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "672ef5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training... This may take a long time on CPU. Use GPU if possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sriharshini\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 02:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.011239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.104900</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sriharshini\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Sriharshini\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 144.8 seconds. Model saved to models/bert_sensitivity\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === TRAINING: run this cell to start fine-tuning ===\n",
    "print('Beginning training... This may take a long time on CPU. Use GPU if possible.')\n",
    "train_start = time.time()\n",
    "trainer.train()\n",
    "trainer.save_model(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "train_end = time.time()\n",
    "print(f'Training finished in {train_end - train_start:.1f} seconds. Model saved to {MODEL_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dc81374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported project modules from src/.\n",
      "Integration helpers ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Integration pipeline helpers: try to import src modules, otherwise use internal fallbacks\n",
    "try:\n",
    "    from src.detection.detection_pipeline import detect_row\n",
    "    from src.policy_engine.policy_validator import load_rules, validate_row\n",
    "    from src.protection.masking import mask_phone, mask_aadhaar\n",
    "    from src.protection.encryption import encrypt_value, load_key\n",
    "    from src.anomaly_detection.anomaly_model import train_anomaly_model, detect_anomalies\n",
    "    from src.reporting.report_generator import save_compliance_report\n",
    "    print('Imported project modules from src/.')\n",
    "except Exception as e:\n",
    "    print('Falling back to inline implementations (src/ not found or import error):', e)\n",
    "    import re, yaml\n",
    "    PAN_RE = re.compile(r'\\b[A-Z]{5}[0-9]{4}[A-Z]\\b')\n",
    "    AADHAAR_RE = re.compile(r'\\b\\d{4}\\s?\\d{4}\\s?\\d{4}\\b')\n",
    "    EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b')\n",
    "    PHONE_RE = re.compile(r'\\b(\\+?\\d{1,3}[-.\\s]?)?(\\d{10})\\b')\n",
    "    def detect_patterns(val):\n",
    "        val = str(val)\n",
    "        out = []\n",
    "        if PAN_RE.search(val): out.append('pan')\n",
    "        if AADHAAR_RE.search(val): out.append('aadhaar')\n",
    "        if EMAIL_RE.search(val): out.append('email')\n",
    "        if PHONE_RE.search(val): out.append('phone')\n",
    "        return out\n",
    "    def detect_row(row):\n",
    "        found = {}\n",
    "        for c in ['name','email','phone','aadhaar','pan','address']:\n",
    "            v = row.get(c, '')\n",
    "            res = detect_patterns(v)\n",
    "            if res: found[c] = res\n",
    "        return found\n",
    "    def load_rules(path='config/policy_rules.yaml'):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            default = {'aadhaar': {'encrypted': True}, 'email': {'allow_plaintext': False}, 'logs': {'contains_sensitive_data': False}}\n",
    "            with open(path, 'w') as f: yaml.dump(default, f)\n",
    "        with open(path) as f: return yaml.safe_load(f)\n",
    "    def validate_row(row, detections, rules):\n",
    "        violations = []\n",
    "        if 'aadhaar' in str(detections).lower() or (row.get('aadhaar') and str(row.get('aadhaar')).strip()):\n",
    "            if rules.get('aadhaar', {}).get('encrypted', False) and 'aadhaar_encrypted' not in row:\n",
    "                violations.append('aadhaar_plaintext_found')\n",
    "        if 'email' in str(detections).lower() and not rules.get('email', {}).get('allow_plaintext', True):\n",
    "            violations.append('email_plaintext_found')\n",
    "        return violations\n",
    "    from cryptography.fernet import Fernet\n",
    "    KEY_FILE = 'config/encryption_key.key'\n",
    "    def generate_key():\n",
    "        k = Fernet.generate_key()\n",
    "        os.makedirs(os.path.dirname(KEY_FILE), exist_ok=True)\n",
    "        with open(KEY_FILE, 'wb') as f: f.write(k)\n",
    "        return k\n",
    "    def load_key():\n",
    "        if not os.path.exists(KEY_FILE): return generate_key()\n",
    "        with open(KEY_FILE, 'rb') as f: return f.read()\n",
    "    def encrypt_value(val):\n",
    "        key = load_key(); f = Fernet(key); return f.encrypt(str(val).encode()).decode()\n",
    "    def mask_phone(p):\n",
    "        s=str(p); return s[:2] + 'XXXXX' + s[-3:] if len(s)>=10 else s\n",
    "    def mask_aadhaar(a):\n",
    "        s=str(a).replace(' ',''); return s[:4] + ' XXXX ' + s[-4:] if len(s)>=12 else a\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    def featurize_logs(df_local):\n",
    "        dfc = df_local.copy(); dfc['access_time'] = pd.to_datetime(dfc['access_time']); dfc['hour'] = dfc['access_time'].dt.hour; dfc['txn_amount'] = dfc['transaction_amount'].fillna(0); dfc['event_code'] = dfc['event_type'].astype('category').cat.codes; X = dfc[['hour','txn_amount','event_code']].fillna(0); return X\n",
    "    def train_anomaly_model(df_local, save_path='models/anomaly_iforest.pkl'):\n",
    "        X = featurize_logs(df_local); model_local = IsolationForest(n_estimators=100, contamination=0.02, random_state=42); model_local.fit(X); joblib.dump(model_local, save_path); return model_local\n",
    "    def detect_anomalies(df_local, model_path='models/anomaly_iforest.pkl'):\n",
    "        model_local = joblib.load(model_path); X = featurize_logs(df_local); preds = model_local.predict(X); df_local['anomaly'] = (preds == -1); return df_local\n",
    "    def save_compliance_report(violations_list, out_path='reports/compliance_report.json'):\n",
    "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "        report = {'summary': {'total_violations': len(violations_list)}, 'violations': violations_list}\n",
    "        with open(out_path, 'w') as f: json.dump(report, f, indent=2)\n",
    "        return out_path\n",
    "print('Integration helpers ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "596f807d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model from models/bert_sensitivity\n",
      "Model ready on cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load saved fine-tuned model (if exists) else use pretrained for inference\n",
    "if os.path.exists(MODEL_DIR) and os.listdir(MODEL_DIR):\n",
    "    print('Loading fine-tuned model from', MODEL_DIR)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "else:\n",
    "    print('Fine-tuned model not found — using pretrained DistilBERT for inference')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print('Model ready on', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "883a13e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running regex detection...\n",
      "Running BERT inference... (may take a while)\n",
      "Sensitivity distribution:\n",
      " sensitivity\n",
      "Highly Sensitive    200\n",
      "Name: count, dtype: int64\n",
      "Applying masking/encryption and validating policies...\n",
      "Saved reports/protected_sample.csv\n",
      "Saved reports/compliance_report.json\n",
      "Training anomaly detection and writing anomalies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sriharshini\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reports/anomalies_detected.csv. Total anomalies: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build text for BERT and run hybrid detection\n",
    "df['text_for_bert'] = df['text']\n",
    "\n",
    "print('Running regex detection...')\n",
    "df['regex_detections'] = df.apply(detect_row, axis=1)\n",
    "\n",
    "print('Running BERT inference... (may take a while)')\n",
    "def bert_predict_batch(texts, tokenizer, model, device, batch_size=32):\n",
    "    preds, confs = [], []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, truncation=True, padding=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "        input_ids = enc['input_ids'].to(device)\n",
    "        attention_mask = enc['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        preds.extend(np.argmax(probs, axis=1).tolist())\n",
    "        confs.extend(np.max(probs, axis=1).tolist())\n",
    "    return preds, confs\n",
    "\n",
    "texts = df['text_for_bert'].astype(str).tolist()\n",
    "bert_preds, bert_confs = bert_predict_batch(texts, tokenizer, model, device, batch_size=32)\n",
    "df['bert_pred'] = bert_preds\n",
    "df['bert_conf'] = bert_confs\n",
    "label_map = {0:'Public', 1:'Sensitive', 2:'Highly Sensitive'}\n",
    "df['bert_label'] = df['bert_pred'].map(label_map)\n",
    "\n",
    "def synthesize_sensitivity(row):\n",
    "    regs = row.get('regex_detections', {})\n",
    "    if (pd.notna(row.get('aadhaar')) and str(row.get('aadhaar')).strip()) or ('aadhaar' in str(regs).lower()):\n",
    "        return 'Highly Sensitive'\n",
    "    if (pd.notna(row.get('pan')) and str(row.get('pan')).strip()) or ('pan' in str(regs).lower()):\n",
    "        return 'Highly Sensitive'\n",
    "    return row.get('bert_label','Public')\n",
    "\n",
    "df['sensitivity'] = df.apply(synthesize_sensitivity, axis=1)\n",
    "print('Sensitivity distribution:\\n', df['sensitivity'].value_counts())\n",
    "\n",
    "# Protection + policy validation\n",
    "print('Applying masking/encryption and validating policies...')\n",
    "load_key()\n",
    "rules = load_rules()\n",
    "violations = []\n",
    "protected_rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    new_row = row.copy()\n",
    "    if row.get('phone'):\n",
    "        new_row['phone_masked'] = mask_phone(row['phone'])\n",
    "    if row.get('aadhaar'):\n",
    "        new_row['aadhaar_masked'] = mask_aadhaar(row['aadhaar'])\n",
    "    if row['sensitivity'] == 'Highly Sensitive':\n",
    "        if row.get('aadhaar'):\n",
    "            new_row['aadhaar_encrypted'] = encrypt_value(str(row['aadhaar']))\n",
    "        if row.get('pan'):\n",
    "            new_row['pan_encrypted'] = encrypt_value(str(row['pan']))\n",
    "    v = validate_row(row, row.get('regex_detections', {}), rules)\n",
    "    if v:\n",
    "        violations.append({'index': int(idx), 'violations': v})\n",
    "    protected_rows.append(new_row)\n",
    "\n",
    "protected_df = pd.DataFrame(protected_rows)\n",
    "protected_df.to_csv('reports/protected_sample.csv', index=False)\n",
    "print('Saved reports/protected_sample.csv')\n",
    "\n",
    "save_compliance_report(violations, out_path='reports/compliance_report.json')\n",
    "print('Saved reports/compliance_report.json')\n",
    "\n",
    "print('Training anomaly detection and writing anomalies...')\n",
    "train_anomaly_model(df)\n",
    "adf = detect_anomalies(df)\n",
    "adf.to_csv('reports/anomalies_detected.csv', index=False)\n",
    "print('Saved reports/anomalies_detected.csv. Total anomalies:', int(adf['anomaly'].sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "316d3e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dashboards/streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "streamlit_code = r'''\n",
    "import streamlit as st\n",
    "import pandas as pd, os, json\n",
    "\n",
    "st.set_page_config(page_title=\"Data Protection Dashboard\", layout=\"wide\")\n",
    "st.title(\"AI-Powered Data Protection Dashboard\")\n",
    "\n",
    "if not os.path.exists(\"reports/protected_sample.csv\"):\n",
    "    st.info(\"Run the pipeline notebook first to produce reports in reports/.\")\n",
    "else:\n",
    "    protected = pd.read_csv(\"reports/protected_sample.csv\")\n",
    "    anomalies = pd.read_csv(\"reports/anomalies_detected.csv\")\n",
    "    with open(\"reports/compliance_report.json\") as f:\n",
    "        compliance = json.load(f)\n",
    "\n",
    "    st.header(\"Protected Sample (preview)\")\n",
    "    st.dataframe(protected.head(100))\n",
    "\n",
    "    st.header(\"Sensitivity distribution\")\n",
    "    st.bar_chart(protected['sensitivity'].value_counts())\n",
    "\n",
    "    st.header(\"Policy Violations\")\n",
    "    st.write(\"Total violations:\", compliance.get(\"summary\",{}).get(\"total_violations\",0))\n",
    "    st.json(compliance.get(\"violations\", []))\n",
    "\n",
    "    st.header(\"Anomalies\")\n",
    "    st.write(\"Total anomalies:\", int(anomalies['anomaly'].sum()))\n",
    "    st.dataframe(anomalies.head(100))\n",
    "\n",
    "    st.markdown(\"## Download Reports\")\n",
    "    st.markdown(\"- reports/protected_sample.csv\")\n",
    "    st.markdown(\"- reports/anomalies_detected.csv\")\n",
    "    st.markdown(\"- reports/compliance_report.json\")\n",
    "'''\n",
    "\n",
    "with open('dashboards/streamlit_app.py','w',encoding='utf-8') as f:\n",
    "    f.write(streamlit_code)\n",
    "print('Wrote dashboards/streamlit_app.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a232ce",
   "metadata": {},
   "source": [
    "\n",
    "## How to run this notebook (Full B version)\n",
    "1. Set `QUICK_RUN=False` for full training (default in this notebook). If you don't have GPU or want a quick test, set `QUICK_RUN=True` and re-run the notebook.\n",
    "2. Run all cells from top to bottom. Training will begin where indicated and can take a long time on CPU.\n",
    "3. After the integration cells complete, the `reports/` folder will contain:\n",
    "   - `protected_sample.csv`\n",
    "   - `anomalies_detected.csv`\n",
    "   - `compliance_report.json`\n",
    "4. Run the Streamlit dashboard from terminal:\n",
    "   ```bash\n",
    "   streamlit run dashboards/streamlit_app.py\n",
    "   ```\n",
    "   Or:\n",
    "   ```bash\n",
    "   python -m streamlit run dashboards/streamlit_app.py\n",
    "   ```\n",
    "5. Open `http://localhost:8501` in your browser to view the dashboard.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
